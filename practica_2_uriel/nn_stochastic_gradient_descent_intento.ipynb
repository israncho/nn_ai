{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(r0=1,r1=3,k=1000):\n",
    "    \"\"\"\n",
    "    Creaci ́on de los datos\n",
    "    \"\"\"\n",
    "    X1 = [np.array([r0*np.cos(t),r0*np.sin(t)]) for t in range(0,k)]\n",
    "    X2 = [np.array([r1*np.cos(t),r1*np.sin(t)]) for t in range(0,k)]\n",
    "    X = np.concatenate((X1,X2))\n",
    "    n,d = X.shape\n",
    "    Y = np.zeros((2*k,2))\n",
    "    Y[0:k] += [1,0]\n",
    "    Y[k:] += [0,1]\n",
    "    noise = np.array([np.random.normal(0,1,2) for i in range(n)])\n",
    "    X += 0.5*noise\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_classification(k=10)\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(X, Y, test_size=0.3)\n",
    "x_train,y_train ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"Nodo super clase con funciones generales\"\"\"\n",
    "    def __init__(self, values):\n",
    "    # Agrega los par ́ametros necesarios\n",
    "        self.values = values\n",
    "        self.grads = None\n",
    "        return\n",
    "        \n",
    "    def __call__(self, *kwargs):\n",
    "        return self.forward(*kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.values) #Valor n ́um del nodo\n",
    "        \n",
    "    def backward(self, consumer_grad=1):\n",
    "        self.grads = consumer_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preactivation(Node):\n",
    "    def __init__(self, parent, input_size, output_size):\n",
    "        self.w = np.random.uniform(0,1,(input_size, output_size)).T\n",
    "        self.b = np.random.uniform(0,1, output_size)\n",
    "\n",
    "        # Guardamos estos parámetros\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parent = parent\n",
    "        self.learning_rate = 0.1\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def forward(self):\n",
    "\n",
    "        values = []\n",
    "        for x in self.parent.values:\n",
    "            values.append(np.dot(self.w, x) + self.b)\n",
    "\n",
    "        self.values = np.array(values)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def backward(self, consumer_grad):\n",
    "        print(consumer_grad)\n",
    "        print(self.values.T)\n",
    "        # self.grad_w = np.dot(consumer_grad, self.values.T)\n",
    "        self.grad_w = np.outer(consumer_grad, self.values.T).T\n",
    "        print(self.grad_w)\n",
    "        self.grad_b = np.dot(consumer_grad, self.values.T)\n",
    "\n",
    "        self.update()\n",
    "\n",
    "        self.parent.backward(consumer_grad)\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        self.w -= self.learning_rate * self.grad_w\n",
    "        self.b -= self.learning_rate * self.grad_b\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Node):\n",
    "    def __init__(self, preactivation_node):\n",
    "        self.parent = preactivation_node\n",
    "        return\n",
    "\n",
    "    def function(self, x):\n",
    "        return\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return\n",
    "\n",
    "    def forward(self):\n",
    "        values = []\n",
    "\n",
    "        for x in self.parent.values:\n",
    "            values.append(self.function(x))\n",
    "\n",
    "        self.values = np.array(values)\n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad):\n",
    "        self.grad = np.multiply(consumer_grad, self.derivative(self.values[0]))\n",
    "        print(self.grad)\n",
    "        self.parent.backward(self.grad)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def function(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def function(self, x):\n",
    "        relu = np.zeros(len(x))\n",
    "        for i in range(len(x)):\n",
    "            if x[i] >= 0:\n",
    "                relu[i] = x[i]\n",
    "        \n",
    "        return relu\n",
    "\n",
    "\n",
    "    def derivative(self, x):\n",
    "        relu_derivative = np.zeros(len(x))\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            if x[i] >= 0:\n",
    "                relu_derivative[i] = x[i]\n",
    "                \n",
    "        return relu_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kronecker_delta(x,y):\n",
    "    if x == y:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "class Softmax(Activation):\n",
    "    def function(self, X):\n",
    "        add = 0\n",
    "        for x in X:\n",
    "            add += np.exp(x)\n",
    "\n",
    "        softmax_x = []\n",
    "        for x in X:\n",
    "            softmax_x.append( np.exp(x)/add )\n",
    "        \n",
    "        softmax_x = np.array(softmax_x)\n",
    "        return softmax_x\n",
    "\n",
    "    def derivative(self, x):\n",
    "        n = len(x)\n",
    "\n",
    "        deriv = np.zeros((n,n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                S = self.function(x)\n",
    "                deriv[i,j] = S[i] * (kronecker_delta(i,j) - S[j])\n",
    "\n",
    "        return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Entropy(Node):\n",
    "    # Error de clasificación binario\n",
    "    def __init__(self, output_node, classes = [0,1]):\n",
    "        self.parent = output_node\n",
    "        self.classes = classes\n",
    "        return None\n",
    "\n",
    "    def forward(self, Y_real):\n",
    "        # Definido por casos para evitar infinitos innecesarios\n",
    "        epsilon = sys.float_info.epsilon\n",
    "\n",
    "        self.real_outputs = Y_real\n",
    "        add = 0\n",
    "        \n",
    "        # E = Σ_c y_c log(y_pred_c), donde c son las clases\n",
    "\n",
    "        # Sumamos sobre todos los datos\n",
    "        for y_pred, y_real in zip(self.parent.values, Y_real):\n",
    "            # Sumamos sobre todas las clases\n",
    "            for c in self.classes:\n",
    "                add -= y_real[c]*np.log(y_pred[c] + epsilon)\n",
    "        \n",
    "        self.value = add\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def backward(self, consumer_grad = 1):\n",
    "        # Esto hace la división -y_real/y_pred pero a cada valor,\n",
    "        # Esto da una matriz de n x 2 donde n es el número de datos\n",
    "        \n",
    "        dL_df = - self.real_outputs / self.parent.values\n",
    "        # dL_df = - self.real_outputs + self.parent.values\n",
    "\n",
    "        self.grad = dL_df * consumer_grad\n",
    "        # print(f\"dL_df = {self.grad}\")\n",
    "        self.parent.backward(self.grad)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, layers, error_node, y_real):\n",
    "        self.layers = layers\n",
    "        self.error = error_node\n",
    "        self.y_real = y_real\n",
    "    \n",
    "    def forward(self):\n",
    "        for layer in self.layers:\n",
    "            layer.forward()\n",
    "            print(layer)\n",
    "\n",
    "        # self.error.forward(self.y_real)\n",
    "\n",
    "    def backward(self):\n",
    "        self.layers[-1].backward(1)\n",
    "        # self.error.backward()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura\n",
    "\n",
    "initial_node = Node([x_train[0]])\n",
    "pre_tanh = Preactivation(initial_node, 2, 3)\n",
    "tanh_layer = Tanh(pre_tanh)\n",
    "\n",
    "pre_relu = Preactivation(tanh_layer, 3, 4)\n",
    "relu_layer = ReLU(pre_relu)\n",
    "\n",
    "pre_soft = Preactivation(relu_layer, 4, 2)\n",
    "softmax_layer = Softmax(pre_soft)\n",
    "\n",
    "error_node = Cross_Entropy(softmax_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "archi = [pre_tanh, tanh_layer, pre_relu, relu_layer, pre_soft, softmax_layer]\n",
    "nn = Neural_Network(archi, error_node, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[151], line 9\u001b[0m, in \u001b[0;36mNeural_Network.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(layer)\n",
      "Cell \u001b[0;32mIn[145], line 20\u001b[0m, in \u001b[0;36mPreactivation.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mvalues)):\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mvalues[i]\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw, x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "nn.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.24851213 -0.24851213]\n",
      " [-0.24851213  0.24851213]]\n",
      "[[ 0.24851213 -0.24851213]\n",
      " [-0.24851213  0.24851213]]\n",
      "[[3.77494537]\n",
      " [3.46324829]]\n",
      "[[ 0.93811971 -0.93811971 -0.93811971  0.93811971]\n",
      " [ 0.86065921 -0.86065921 -0.86065921  0.86065921]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[136], line 15\u001b[0m, in \u001b[0;36mNeural_Network.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[131], line 24\u001b[0m, in \u001b[0;36mActivation.backward\u001b[0;34m(self, consumer_grad)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(consumer_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[130], line 30\u001b[0m, in \u001b[0;36mPreactivation.backward\u001b[0;34m(self, consumer_grad)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mouter(consumer_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_w)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_b \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsumer_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mbackward(consumer_grad)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
