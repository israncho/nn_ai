{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(r0=1,r1=3,k=1000):\n",
    "    \"\"\"\n",
    "    Creaci ́on de los datos\n",
    "    \"\"\"\n",
    "    X1 = [np.array([r0*np.cos(t),r0*np.sin(t)]) for t in range(0,k)]\n",
    "    X2 = [np.array([r1*np.cos(t),r1*np.sin(t)]) for t in range(0,k)]\n",
    "    X = np.concatenate((X1,X2))\n",
    "    n,d = X.shape\n",
    "    Y = np.zeros((2*k,2))\n",
    "    Y[0:k] += [1,0]\n",
    "    Y[k:] += [0,1]\n",
    "    noise = np.array([np.random.normal(0,1,2) for i in range(n)])\n",
    "    X += 0.5*noise\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.3607577 ,  0.53242367],\n",
       "        [-1.82562063,  1.62122554],\n",
       "        [-0.93510202,  2.6111466 ],\n",
       "        [-1.42589537,  1.01701671],\n",
       "        [ 1.42508044,  0.5959117 ],\n",
       "        [-0.98742896,  0.08100617],\n",
       "        [ 0.5867844 , -2.89637741],\n",
       "        [ 2.13475472,  2.00758583],\n",
       "        [-0.01524927, -0.72269578],\n",
       "        [ 2.98799883, -1.0304259 ],\n",
       "        [ 1.23649292, -0.34071294],\n",
       "        [-1.05713456,  0.6791072 ],\n",
       "        [ 1.50814835, -0.8869999 ],\n",
       "        [-0.03957542,  0.14193676]]),\n",
       " array([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = make_classification(k=10)\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(X, Y, test_size=0.3)\n",
    "x_train,y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"Nodo super clase con funciones generales\"\"\"\n",
    "    def __init__(self, values):\n",
    "    # Agrega los par ́ametros necesarios\n",
    "        self.values = values\n",
    "        self.grads = None\n",
    "        return\n",
    "        \n",
    "    def __call__(self, *kwargs):\n",
    "        return self.forward(*kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.values) #Valor n ́um del nodo\n",
    "        \n",
    "    def backward(self, consumer_grad=1):\n",
    "        self.grads = consumer_grad\n",
    "# Agrega otros m ́etodos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clases basicas\n",
    "\n",
    "class PreActivation(Node):\n",
    "    # Pre-activación wx+b\n",
    "    def __init__(self, input_size, output_size, parent = None):\n",
    "        # Generamos una matriz aleatoria de tamaño input x output \n",
    "        # Y la trasponemos para usarla como matriz\n",
    "        self.w = np.random.uniform(0,1,(input_size, output_size)).T\n",
    "        \n",
    "        # Generamos el vector de sesgo del tamaño de salida\n",
    "        self.b = np.random.uniform(0,1, output_size).T # Bias\n",
    "        print(self.w)\n",
    "        print(self.b)\n",
    "        # Guardamos estos parámetros\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parent = parent\n",
    "\n",
    "        return None\n",
    "\n",
    "    def forward(self):\n",
    "        self.values = np.dot(self.w, self.parent.values) + self.b\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad=1):\n",
    "        # La función backward recibe el gradiente de los nodos hijos y regresa el gradiente\n",
    "        self.grad_w = np.dot(consumer_grad, self.values.T)\n",
    "        self.grad_b = consumer_grad\n",
    "\n",
    "        self.update()\n",
    "        return self  \n",
    "\n",
    "    def update(self):\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Node):\n",
    "    # Activación ReLU\n",
    "\n",
    "    def __init__(self, preactivation_node):\n",
    "        # Usualmente un nodo de preactivación\n",
    "        self.parent = preactivation_node\n",
    "        return None\n",
    "\n",
    "    def function(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward(self):\n",
    "        self.values =  self.function(self.parent.values)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad):\n",
    "        m = self.preactivation.output_size\n",
    "        \n",
    "        dh_da = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            dh_da[i] = 1 - self.value[i]**2\n",
    "\n",
    "        d_k = dh_da * consumer_grad\n",
    "        self.grad = d_k\n",
    "        print(f\"Tanh d_k = {d_k}\")\n",
    "        # Calculamos la suma d_k+1 * W_q,i para pasarla al siguiente nodo\n",
    "        n = self.preactivation.input_size\n",
    "        d_kW = np.zeros(m)\n",
    "        for i in range(n):\n",
    "            for q in range(m):\n",
    "                d_kW[q] += d_k[q] * self.preactivation.w[q][i]\n",
    "\n",
    "        self.parent.backward(d_kW)\n",
    "\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Node):\n",
    "    # Activación ReLU\n",
    "\n",
    "    def __init__(self, preactivation_node):\n",
    "        # Nodo de preactivación\n",
    "        self.parent = preactivation_node\n",
    "        return None\n",
    "\n",
    "    def function(self, x):\n",
    "            return x * (x >= 0)\n",
    "\n",
    "    def forward(self):\n",
    "        self.values = self.function(self.parent.values)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad):\n",
    "        m = self.preactivation.output_size\n",
    "\n",
    "        dh_da = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            # Calculamos la derivada de ReLu respecto a su preactivación a\n",
    "            if self.preactivation.value[i] >= 0:\n",
    "                dh_da[i] = 1\n",
    "            else:\n",
    "                dh_da[i] = 0\n",
    "        \n",
    "        d_k = dh_da * consumer_grad\n",
    "        self.grad = d_k\n",
    "        print(f\"ReLu d_k = {d_k}\")\n",
    "        # Calculamos la suma d_k+1 * W_q,i para pasarla al siguiente nodo\n",
    "\n",
    "        n = self.preactivation.input_size\n",
    "        d_kW = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            for q in range(m):\n",
    "                d_kW[q] += d_k[q] * self.preactivation.w[q][i]\n",
    "            \n",
    "        self.parent.backward(d_kW)\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kronecker_delta(x,y):\n",
    "    if x == y:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class Softmax(Node):\n",
    "    # Activación ReLU\n",
    "\n",
    "    def __init__(self, preactivation_node):\n",
    "        # Nodo de preactivación\n",
    "        self.parent = preactivation_node\n",
    "        return None\n",
    "\n",
    "    def function(self, x):\n",
    "        # Recibimos sólo un arreglo x = [x_1, x_2] para evaluar en la softmax\n",
    "        S = sum( [np.exp(x_i) for x_i in x] )\n",
    "        return np.exp(x) / S\n",
    "\n",
    "    def forward(self):\n",
    "        # self.parent = x\n",
    "        # Aplicamos softmax a cada renglón de la matriz de preactivación values\n",
    "        # Esto nos da una matriz de n x m siendo n el número de datos y m las capas ocultas\n",
    "        self.values = self.function(self.parent.values)\n",
    "        return self\n",
    "\n",
    "    def derivative(self, X):\n",
    "        # En este caso X son todos los datos\n",
    "\n",
    "        # Tomamos la dimensión del problema\n",
    "        classes = len(X[0]) \n",
    "\n",
    "        # inicializamos el arreglo de derivadas\n",
    "        derivatives = []\n",
    "\n",
    "        # Calculamos la derivada de la softmax para cada valor s\n",
    "        for s in X:\n",
    "            DS = np.zeros((classes, classes)) # Inicializamos la matriz derivada\n",
    "            for i in range(classes):\n",
    "                for j in range(classes):\n",
    "                    # Recordemos que s ya son softmax\n",
    "                    DS[i][j] = s[i] * (kronecker_delta(i,j) - s[j])\n",
    "\n",
    "            # Agregamos esta matriz al arreglo\n",
    "            derivatives.append( DS )\n",
    "\n",
    "        # Las derivadas es un arreglo de matrices cuadradas del tamaño de las salidas de la capa\n",
    "        derivatives = np.array(derivatives) # Lo hacemos tipo numpy\n",
    "        return derivatives\n",
    "\n",
    "\n",
    "    def backward(self, consumer_grad):\n",
    "        d_k = np.multiply(consumer_grad, self.derivative(self.values))\n",
    "        \n",
    "\n",
    "        # dh_da = self.derivative(self.values)\n",
    "        # print(f\"dh_da = {dh_da}\")\n",
    "    \n",
    "        # d_k = []\n",
    "        # for i in range(len(dh_da)):\n",
    "        #     d_k.append( np.dot(dh_da[i], consumer_grad[i]) )\n",
    "\n",
    "        # d_k = np.array(d_k)\n",
    "        # # d_k = np.dot(dh_da.T, consumer_grad)\n",
    "        # self.grad = d_k\n",
    "        print(f\"d_k = {d_k}\")\n",
    "\n",
    "\n",
    "        # Calculamos la suma d_k+1 * W_q,i para pasarla al siguiente nodo\n",
    "\n",
    "        # n = self.parent.input_size\n",
    "        # d_kW = np.zeros((n,len(d_k)))\n",
    "        # for i in range(n):\n",
    "        #     for q in range(len(d_k)):\n",
    "        #         d_kW[q][:] += np.dote(d_k[q], self.parent.w[q][i])\n",
    "\n",
    "        # print(d_kW)\n",
    "        # self.parent.backward(d_kW)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Node):\n",
    "    # Error de clasificación binario\n",
    "    def __init__(self, output_node, classes = [0,1]):\n",
    "        self.parent = output_node\n",
    "        self.classes = classes\n",
    "        return None\n",
    "\n",
    "    def forward(self, Y_real):\n",
    "        # Definido por casos para evitar infinitos innecesarios\n",
    "        epsilon = sys.float_info.epsilon\n",
    "\n",
    "        self.real_outputs = Y_real\n",
    "        add = 0\n",
    "        \n",
    "        # E = Σ_c y_c log(y_pred_c), donde c son las clases\n",
    "\n",
    "        # Sumamos sobre todos los datos\n",
    "        for y_pred, y_real in zip(self.parent.values, Y_real):\n",
    "            # Sumamos sobre todas las clases\n",
    "            for c in self.classes:\n",
    "                add -= y_real[c]*np.log(y_pred[c] + epsilon)\n",
    "        \n",
    "        self.value = add\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def backward(self, consumer_grad = 1):\n",
    "        # Esto hace la división -y_real/y_pred pero a cada valor,\n",
    "        # Esto da una matriz de n x 2 donde n es el número de datos\n",
    "        \n",
    "        dL_df = - self.real_outputs / self.parent.values\n",
    "        # dL_df = - self.real_outputs + self.parent.values\n",
    "\n",
    "        self.grad = dL_df * consumer_grad\n",
    "        # print(f\"dL_df = {self.grad}\")\n",
    "        self.parent.backward(self.grad)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81465939 0.56870448]\n",
      " [0.84753015 0.36016433]\n",
      " [0.75472427 0.6222704 ]\n",
      " [0.20482329 0.9801781 ]]\n",
      "[0.79918599 0.92758313 0.37418154 0.58505709]\n",
      "[[1.31800684e-01 1.92974832e-01 1.47200029e-01 8.96373524e-01]\n",
      " [4.44618356e-04 9.03643223e-01 9.42721860e-01 1.44522347e-01]\n",
      " [7.07492575e-01 3.19596741e-01 1.90922764e-01 4.38372896e-01]]\n",
      "[0.86187837 0.85397894 0.71046179]\n",
      "[[0.91556179 0.80581815 0.62590033]\n",
      " [0.79514374 0.4433964  0.93225286]]\n",
      "[0.16343504 0.7490742 ]\n"
     ]
    }
   ],
   "source": [
    "# Arquitectura de la Red\n",
    "initial_node = Node(x_train)\n",
    "\n",
    "pre_tanh = PreActivation(2, 4, initial_node)\n",
    "tanh_layer = Tanh( pre_tanh )\n",
    "pre_relu = PreActivation(4, 3, tanh_layer)\n",
    "relu_layer = ReLU(pre_relu)\n",
    "pre_soft = PreActivation(3, 2, relu_layer)\n",
    "softmax_layer = Softmax(pre_soft)\n",
    "error = CrossEntropy(softmax_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.88445618  0.89065725  0.75209704  0.82770959]\n",
      " [ 0.22974972 -0.03576252  0.00518195  0.94682851]\n",
      " [ 0.90910929  0.79152356  0.85998227  0.99456776]\n",
      " [ 0.2126535   0.08518005 -0.06900706  0.85908935]\n",
      " [ 0.98005848  0.98197369  0.94889249  0.89785563]\n",
      " [ 0.0408136   0.11931182 -0.31009211  0.431883  ]\n",
      " [-0.35396321  0.36420787 -0.75534591 -0.97235239]\n",
      " [ 0.99872843  0.99802595  0.99690383  0.99495607]\n",
      " [ 0.35902196  0.57460425 -0.0868205  -0.12576742]\n",
      " [ 0.99001446  0.99585845  0.96317663  0.18491594]\n",
      " [ 0.92356414  0.95201228  0.79883234  0.46553936]\n",
      " [ 0.31329304  0.26940292 -0.00107526  0.77557831]\n",
      " [ 0.90928356  0.95505074  0.74448373  0.02453819]\n",
      " [ 0.68984794  0.73758508  0.40752194  0.61447182]]\n",
      "[[ 2.0029704   2.48784942  2.12729702]\n",
      " [ 1.73473306  0.96348755  1.27763173]\n",
      " [ 2.15253734  2.5240994   2.20679995]\n",
      " [ 1.66625096  0.99014901  1.2515623 ]\n",
      " [ 2.12503797  2.76607045  2.29244223]\n",
      " [ 1.23176472  0.73189854  0.90759093]\n",
      " [-0.          0.33032778  0.00596943]\n",
      " [ 2.2247019   2.8398788   2.36251399]\n",
      " [ 0.8945673   1.27335201  1.07639977]\n",
      " [ 1.49207194  2.68905201  1.99411726]\n",
      " [ 1.70220447  2.53502658  2.02473171]\n",
      " [ 1.65020817  1.20863709  1.35800181]\n",
      " [ 1.29760673  2.42279576  1.81190003]\n",
      " [ 1.70591967  1.99378417  1.78142688]]\n",
      "[[0.11805704 0.11876348]\n",
      " [0.01588564 0.02210596]\n",
      " [0.14650762 0.1463866 ]\n",
      " [0.0149975  0.02067473]\n",
      " [0.18318348 0.17269283]\n",
      " [0.00659749 0.00947122]\n",
      " [0.00087899 0.00128433]\n",
      " [0.22253197 0.20619299]\n",
      " [0.0083306  0.01077888]\n",
      " [0.08001261 0.07639762]\n",
      " [0.08732341 0.08677193]\n",
      " [0.01883803 0.02483519]\n",
      " [0.0482085  0.04907718]\n",
      " [0.04864712 0.05456708]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(46.84968892406513)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0 = pre_tanh()\n",
    "l_0 = tanh_layer()\n",
    "print(l_0)\n",
    "\n",
    "p_1 = pre_relu()\n",
    "l_1 = relu_layer()\n",
    "print(l_1)\n",
    "\n",
    "p_2 = pre_soft()\n",
    "l_2= softmax_layer()\n",
    "print(l_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "error(y_train).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.3607577 , 0.53242367]), array([1., 0.]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (14,2) (14,2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[141], line 36\u001b[0m, in \u001b[0;36mCrossEntropy.backward\u001b[0;34m(self, consumer_grad)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m dL_df \u001b[38;5;241m*\u001b[39m consumer_grad\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(f\"dL_df = {self.grad}\")\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[140], line 54\u001b[0m, in \u001b[0;36mSoftmax.backward\u001b[0;34m(self, consumer_grad)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, consumer_grad):\n\u001b[0;32m---> 54\u001b[0m     d_k \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsumer_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderivative\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# dh_da = self.derivative(self.values)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# print(f\"dh_da = {dh_da}\")\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# # d_k = np.dot(dh_da.T, consumer_grad)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# self.grad = d_k\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_k = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (14,2) (14,2,2) "
     ]
    }
   ],
   "source": [
    "error.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
