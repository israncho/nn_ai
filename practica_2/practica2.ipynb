{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(r0=1,r1=3,k=1000):\n",
    "    \"\"\"\n",
    "    Creaci ́on de los datos\n",
    "    \"\"\"\n",
    "    X1 = [np.array([r0*np.cos(t),r0*np.sin(t)]) for t in range(0,k)]\n",
    "    X2 = [np.array([r1*np.cos(t),r1*np.sin(t)]) for t in range(0,k)]\n",
    "    X = np.concatenate((X1,X2))\n",
    "    n,d = X.shape\n",
    "    Y = np.zeros((2*k,2))\n",
    "    Y[0:k] += [1,0]\n",
    "    Y[k:] += [0,1]\n",
    "    noise = np.array([np.random.normal(0,1,2) for i in range(n)])\n",
    "    X += 0.5*noise\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.76426676,  0.16091609],\n",
       "        [-0.61529766, -0.74541055],\n",
       "        [-0.3287628 , -0.70257991],\n",
       "        [ 1.01693482, -0.17293928],\n",
       "        [-1.40265409,  2.40347388],\n",
       "        [-3.0017048 ,  1.1205725 ],\n",
       "        [-0.38794572,  1.7246724 ],\n",
       "        [ 0.409174  ,  2.50235795],\n",
       "        [ 0.07288647, -0.51099316],\n",
       "        [ 0.87497902,  0.48022502],\n",
       "        [ 3.01475757, -0.54734915],\n",
       "        [ 2.7509809 , -0.27612794],\n",
       "        [ 2.65890006,  2.04163995],\n",
       "        [-2.51613602,  1.46012579]]),\n",
       " array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = make_classification(k=10)\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(X, Y, test_size=0.3)\n",
    "x_train,y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"Nodo super clase con funciones generales\"\"\"\n",
    "    def __init__(self, values):\n",
    "    # Agrega los par ́ametros necesarios\n",
    "        self.values = values\n",
    "        self.grads = None\n",
    "        return\n",
    "        \n",
    "    def __call__(self, *kwargs):\n",
    "        return self.forward(*kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.values) #Valor n ́um del nodo\n",
    "        \n",
    "    def backward(self, consumer_grad=1):\n",
    "        self.grads = consumer_grad\n",
    "# Agrega otros m ́etodos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clases basicas\n",
    "\n",
    "class PreActivation(Node):\n",
    "    # Pre-activación wx+b\n",
    "    def __init__(self, input_size, output_size, parent = None):\n",
    "        # Generamos una matriz aleatoria de tamaño input x output \n",
    "        # Y la trasponemos para usarla como matriz\n",
    "        self.w = np.random.uniform(0,1,(input_size, output_size)).T\n",
    "        \n",
    "        # Generamos el vector de sesgo del tamaño de salida\n",
    "        self.b = np.random.uniform(0,1, output_size).T # Bias\n",
    "        print(self.w)\n",
    "        print(self.b)\n",
    "        # Guardamos estos parámetros\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parent = parent\n",
    "\n",
    "        return None\n",
    "\n",
    "    def forward(self):\n",
    "        # Función que activa el nodo\n",
    "\n",
    "        # A cada valor x le aplicamos Wx + b\n",
    "        # Tenemos una matriz donde cada renglón es un vector de preactivación a\n",
    "        self.values = []\n",
    "        for value in self.parent.values:\n",
    "            # print(np.dot(self.w, value) + self.b)\n",
    "            self.values.append(np.dot(self.w, value) + self.b) # Expresión lineal\n",
    "        \n",
    "        self.values = np.array(self.values)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad=1):\n",
    "        # La función backward recibe el gradiente de los nodos hijos y regresa el gradiente\n",
    "        self.grad_w = self.value * consumer_grad\n",
    "        self.grad_b = consumer_grad\n",
    "\n",
    "        self.update()\n",
    "        return self  \n",
    "\n",
    "    def update(self):\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Node):\n",
    "    # Activación ReLU\n",
    "\n",
    "    def __init__(self, preactivation_node):\n",
    "        # Usualmente un nodo de preactivación\n",
    "        self.parent = preactivation_node\n",
    "        return None\n",
    "\n",
    "    def function(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward(self):\n",
    "        self.values =  self.function(self.parent.values)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad):\n",
    "        m = self.preactivation.output_size\n",
    "        \n",
    "        dh_da = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            dh_da[i] = 1 - self.value[i]**2\n",
    "\n",
    "        d_k = dh_da * consumer_grad\n",
    "        self.grad = d_k\n",
    "        print(f\"Tanh d_k = {d_k}\")\n",
    "        # Calculamos la suma d_k+1 * W_q,i para pasarla al siguiente nodo\n",
    "        n = self.preactivation.input_size\n",
    "        d_kW = np.zeros(m)\n",
    "        for i in range(n):\n",
    "            for q in range(m):\n",
    "                d_kW[q] += d_k[q] * self.preactivation.w[q][i]\n",
    "\n",
    "        self.parent.backward(d_kW)\n",
    "\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Node):\n",
    "    # Activación ReLU\n",
    "\n",
    "    def __init__(self, preactivation_node):\n",
    "        # Nodo de preactivación\n",
    "        self.parent = preactivation_node\n",
    "        return None\n",
    "\n",
    "    def function(self, x):\n",
    "            return x * (x >= 0)\n",
    "\n",
    "    def forward(self):\n",
    "        self.values = self.function(self.parent.values)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad):\n",
    "        m = self.preactivation.output_size\n",
    "\n",
    "        dh_da = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            # Calculamos la derivada de ReLu respecto a su preactivación a\n",
    "            if self.preactivation.value[i] >= 0:\n",
    "                dh_da[i] = 1\n",
    "            else:\n",
    "                dh_da[i] = 0\n",
    "        \n",
    "        d_k = dh_da * consumer_grad\n",
    "        self.grad = d_k\n",
    "        print(f\"ReLu d_k = {d_k}\")\n",
    "        # Calculamos la suma d_k+1 * W_q,i para pasarla al siguiente nodo\n",
    "\n",
    "        n = self.preactivation.input_size\n",
    "        d_kW = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            for q in range(m):\n",
    "                d_kW[q] += d_k[q] * self.preactivation.w[q][i]\n",
    "            \n",
    "        self.parent.backward(d_kW)\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Node):\n",
    "    # Activación ReLU\n",
    "\n",
    "    def __init__(self, preactivation_node):\n",
    "        # Nodo de preactivación\n",
    "        self.parent = preactivation_node\n",
    "        return None\n",
    "\n",
    "    def function(self, x):\n",
    "        S = sum( [np.exp(x_i) for x_i in x] )\n",
    "        return np.exp(x) / S\n",
    "\n",
    "    def forward(self):\n",
    "        # self.parent = x\n",
    "        #Aplicamos softmax a cada renglón de la matriz de preactivación values\n",
    "        self.values = self.function(self.parent.values)\n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad):\n",
    "        m = self.preactivation.output_size\n",
    "\n",
    "        df_da = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            x = self.value[i]\n",
    "            df_da[i] =  x*(1-x)\n",
    "\n",
    "        d_k = consumer_grad * df_da\n",
    "        print(f\"softmax d_k = {d_k}\")\n",
    "        self.grad = d_k\n",
    "\n",
    "        # Calculamos la suma d_k+1 * W_q,i para pasarla al siguiente nodo\n",
    "        n = self.preactivation.input_size\n",
    "        d_kW = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            for q in range(m):\n",
    "                d_kW[q] += d_k[q] * self.preactivation.w[q][i]\n",
    "\n",
    "        self.parent.backward(d_kW)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Node):\n",
    "    # Error de clasificación binario\n",
    "    def __init__(self, output_node, classes = [0,1]):\n",
    "        self.parent = output_node\n",
    "        self.classes = classes\n",
    "        return None\n",
    "\n",
    "    def forward(self, Y_real):\n",
    "        # Definido por casos para evitar infinitos innecesarios\n",
    "        epsilon = sys.float_info.epsilon\n",
    "\n",
    "        self.real_output = Y_real\n",
    "        add = 0\n",
    "\n",
    "        # E = 0*log(p_0) + 1*log(p_1)\n",
    "        for y_pred, y_real in zip(self.parent.values, Y_real):\n",
    "            for c in self.classes:\n",
    "                add -= y_real[c]*np.log(y_pred[c] + epsilon)\n",
    "        \n",
    "        self.value = add\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def backward(self, consumer_grad = 1):\n",
    "        dL_df = - self.real_output / self.parent.value\n",
    "        self.grad = dL_df * consumer_grad\n",
    "        print(f\"dL_df = {self.grad}\")\n",
    "        self.parent.backward(self.grad)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39896034 0.13918484]\n",
      " [0.21362927 0.04570622]\n",
      " [0.14313454 0.49488973]\n",
      " [0.4766943  0.64167011]]\n",
      "[0.40498166 0.19440978 0.15859723 0.31379152]\n",
      "[[0.86654101 0.76444474 0.48707278 0.13560891]\n",
      " [0.23352597 0.28595968 0.71714878 0.65733534]\n",
      " [0.40333586 0.29580364 0.4945167  0.41556978]]\n",
      "[0.83297728 0.10599079 0.16138678]\n",
      "[[0.93759214 0.36640151 0.35875738]\n",
      " [0.69292549 0.56456855 0.88460132]]\n",
      "[0.42355058 0.89331428]\n"
     ]
    }
   ],
   "source": [
    "# Arquitectura de la Red\n",
    "initial_node = Node(x_train)\n",
    "\n",
    "pre_tanh = PreActivation(2, 4, initial_node)\n",
    "tanh_layer = Tanh( pre_tanh )\n",
    "pre_relu = PreActivation(4, 3, tanh_layer)\n",
    "relu_layer = ReLU(pre_relu)\n",
    "pre_soft = PreActivation(3, 2, relu_layer)\n",
    "softmax_layer = Softmax(pre_soft)\n",
    "error = CrossEntropy(softmax_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12185801  0.03847589  0.12813177  0.05267616]\n",
      " [ 0.05569475  0.02888624 -0.28981946 -0.4283099 ]\n",
      " [ 0.17423394  0.09180492 -0.23186513 -0.28558482]\n",
      " [ 0.65649444  0.38315509  0.21515454  0.59643063]\n",
      " [ 0.1779893   0.00461548  0.81685216  0.82976817]\n",
      " [-0.56258883 -0.37619943  0.27614999 -0.37829308]\n",
      " [ 0.45441876  0.18809491  0.74275262  0.84417612]\n",
      " [ 0.72424512  0.37668826  0.89678592  0.97128614]\n",
      " [ 0.34779925  0.18448801 -0.08365947  0.02064412]\n",
      " [ 0.6755614   0.38275211  0.47885333  0.77750674]\n",
      " [ 0.91069248  0.67147971  0.30881568  0.88528499]\n",
      " [ 0.89844233  0.64662643  0.39330575  0.8952938 ]\n",
      " [ 0.94136901  0.69405853  0.91371358  0.99385794]\n",
      " [-0.37620315 -0.26954555  0.47851193  0.051238  ]]\n",
      "[[ 1.03753778  0.27196579  0.30717171]\n",
      " [ 0.7040752  -0.36212964 -0.12891809]\n",
      " [ 0.90247488 -0.18107535  0.0254762 ]\n",
      " [ 1.88043476  0.91521894  0.89376926]\n",
      " [ 1.50113099  1.28011622  0.98331513]\n",
      " [ 0.14109255 -0.18359096 -0.19745302]\n",
      " [ 1.84679031  1.35346785  1.11842697]\n",
      " [ 2.31703779  1.66442811  1.41203934]\n",
      " [ 1.23744175  0.1935409   0.32344698]\n",
      " [ 2.04964503  1.22769532  1.10699365]\n",
      " [ 2.40590702  1.31407314  1.24794005]\n",
      " [ 2.41880291  1.37127648  1.28158855]\n",
      " [ 2.75910252  1.83286417  1.61124364]\n",
      " [ 0.54094761  0.31790322  0.1878431 ]]\n",
      "[[ 1.03753778  0.27196579  0.30717171]\n",
      " [ 0.7040752  -0.         -0.        ]\n",
      " [ 0.90247488 -0.          0.0254762 ]\n",
      " [ 1.88043476  0.91521894  0.89376926]\n",
      " [ 1.50113099  1.28011622  0.98331513]\n",
      " [ 0.14109255 -0.         -0.        ]\n",
      " [ 1.84679031  1.35346785  1.11842697]\n",
      " [ 2.31703779  1.66442811  1.41203934]\n",
      " [ 1.23744175  0.1935409   0.32344698]\n",
      " [ 2.04964503  1.22769532  1.10699365]\n",
      " [ 2.40590702  1.31407314  1.24794005]\n",
      " [ 2.41880291  1.37127648  1.28158855]\n",
      " [ 2.75910252  1.83286417  1.61124364]\n",
      " [ 0.54094761  0.31790322  0.1878431 ]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Softmax' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[694], line 17\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(l_1)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# p_2 = pre_soft()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# l_2= softmax_layer()\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(l_2)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalue\n",
      "Cell \u001b[0;32mIn[527], line 10\u001b[0m, in \u001b[0;36mNode.__call__\u001b[0;34m(self, *kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[669], line 16\u001b[0m, in \u001b[0;36mCrossEntropy.forward\u001b[0;34m(self, Y_real)\u001b[0m\n\u001b[1;32m     13\u001b[0m add \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# E = 0*log(p_0) + 1*log(p_1)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y_pred, y_real \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m, Y_real):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses:\n\u001b[1;32m     18\u001b[0m         add \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m y_real[c]\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(y_pred[c] \u001b[38;5;241m+\u001b[39m epsilon)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Softmax' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "p_0 = pre_tanh()\n",
    "l_0 = tanh_layer()\n",
    "print(l_0)\n",
    "\n",
    "p_1 = pre_relu()\n",
    "l_1 = relu_layer()\n",
    "print(l_1)\n",
    "\n",
    "# p_2 = pre_soft()\n",
    "# l_2= softmax_layer()\n",
    "# print(l_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "error(y_train).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.76426676,  0.16091609]), array([1., 0.]))"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tanh' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[560], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[556], line 21\u001b[0m, in \u001b[0;36mCrossEntropy.backward\u001b[0;34m(self, consumer_grad)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, consumer_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     dL_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreal_output \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m dL_df \u001b[38;5;241m*\u001b[39m consumer_grad\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdL_df = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tanh' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "error.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
